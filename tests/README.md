# Chives Backend Test Suite

This directory contains comprehensive tests for the chives-backend application components.

## ğŸ§ª Test Structure

```
tests/
â”œâ”€â”€ __init__.py                     # Package initialization
â”œâ”€â”€ conftest.py                     # Pytest configuration and fixtures
â”œâ”€â”€ run_tests.py                    # Interactive test runner
â”œâ”€â”€ test_main.py                    # FastAPI application tests
â”œâ”€â”€ test_config.py                  # Configuration and settings tests
â”œâ”€â”€ test_openapi_chat_model.py      # LLM integration tests
â”œâ”€â”€ test_chat_agent.py              # Chat agent functionality tests
â”œâ”€â”€ test_tools.py                   # Tool implementation tests
â”œâ”€â”€ test_chunking.py                # Chunking strategy tests
â””â”€â”€ README.md                       # This file
```

## ğŸš€ Running Tests

### Option 1: Direct Poetry Commands
```bash
# Run all tests
poetry run pytest tests/ -v

# Run with coverage
poetry run pytest tests/ --cov=chives_backend --cov=ai_core

# Run specific test file
poetry run pytest tests/test_main.py -v

# Run with HTML coverage report
poetry run pytest tests/ --cov=chives_backend --cov=ai_core --cov-report=html
```

### Option 2: Direct Pytest (if you have pytest installed)
```bash
# Set required environment variables first
export OPENAPI_LLM_URL="http://test-api.com"
export OPENAPI_LLM_API_KEY="test-key"
export OPENAPI_LLM_MODEL="test-model"

pytest tests/ -v
```

### Option 3: Using Test Categories
```bash
# Run only integration tests
poetry run pytest tests/ -m integration

# Run only unit tests (exclude integration)
poetry run pytest tests/ -m "not integration"

# Run fast tests only (exclude slow tests)
poetry run pytest tests/ -m "not slow"
```

## ğŸ“‹ Test Coverage

### **FastAPI Application** (`test_main.py`)
- âœ… Root endpoint (`/`)
- âœ… Health check endpoint (`/health`) 
- âœ… Generate endpoint (`/generate`)
- âœ… Error handling and validation
- âœ… CORS middleware configuration
- âœ… Async endpoint functionality
- âœ… Application initialization

### **Configuration** (`test_config.py`)
- âœ… Settings validation and defaults
- âœ… Environment variable loading
- âœ… Required field validation
- âœ… Type conversion (int, bool, list)
- âœ… Error handling for missing config
- âœ… Settings singleton behavior

### **LLM Integration** (`test_openapi_chat_model.py`)
- âœ… Model initialization and validation
- âœ… API request/response handling
- âœ… Tool binding and formatting
- âœ… Message conversion (LangChain â†” OpenAI)
- âœ… Error handling (HTTP, JSON, timeout, connection)
- âœ… Tool invocation with structured schemas

### **Chat Agent** (`test_chat_agent.py`)
- âœ… Agent initialization and configuration
- âœ… Message processing (sync and async)
- âœ… Tool execution and error handling
- âœ… RAG integration and document retrieval
- âœ… Conversation history management
- âœ… Statistics and performance tracking

### **Tool System** (`test_tools.py`)
- âœ… BaseTool abstract class functionality
- âœ… TestTool implementation
- âœ… Argument validation with Pydantic
- âœ… LangChain compatibility
- âœ… Error handling and custom validation
- âœ… Tool serialization and schemas

### **Chunking Strategies** (`test_chunking.py`)
- âœ… Chunk data class and metadata handling
- âœ… ChunkingStrategy abstract base class
- âœ… FixedSizeChunker with overlap and boundary respect
- âœ… TokenBasedFixedSizeChunker with tiktoken integration
- âœ… SemanticChunker with embedding-based splitting
- âœ… SemanticChunkingConfig configuration management
- âœ… Content preservation and metadata consistency
- âœ… Integration testing across strategies

## ğŸ”§ Test Configuration

### Fixtures (`conftest.py`)
The test suite includes comprehensive fixtures for:

- **Mock Settings**: Test configuration with safe defaults
- **Mock LLM**: Mocked OpenAPIChatModel for isolated testing
- **Mock Tools**: Test tools that don't make external calls
- **Mock Chat Agent**: Configured agent with predictable responses
- **FastAPI Test Client**: Fully configured test client
- **Sample Data**: Message history, tool calls, and test payloads

### Environment Setup
Tests automatically set up safe test environment variables and clean up after each test.

### Async Testing
Full support for async testing with `pytest-asyncio` plugin.

## ğŸ“Š Coverage Reports

Generate detailed coverage reports:

```bash
# Terminal coverage report
poetry run pytest tests/ --cov=chives_backend --cov=ai_core --cov-report=term

# HTML coverage report
poetry run pytest tests/ --cov=chives_backend --cov=ai_core --cov-report=html

# Both terminal and HTML
poetry run pytest tests/ --cov=chives_backend --cov=ai_core --cov-report=term --cov-report=html
```

HTML reports are generated in the `htmlcov/` directory.

## ğŸ·ï¸ Test Markers

The test suite uses pytest markers for categorization:

- `integration`: Tests that involve multiple components or external systems
- `slow`: Tests that take more time to execute
- Custom markers can be added in `pyproject.toml`

## ğŸ› ï¸ Dependencies

Test dependencies are managed in `pyproject.toml`:

- `pytest`: Core testing framework
- `pytest-asyncio`: Async test support
- `pytest-mock`: Enhanced mocking capabilities
- `httpx`: Async HTTP client for FastAPI testing

## ğŸš¨ Common Issues & Solutions

### Environment Variables
If you see validation errors about missing environment variables:
```bash
export OPENAPI_LLM_URL="http://test-api.com"
export OPENAPI_LLM_API_KEY="test-key"
export OPENAPI_LLM_MODEL="test-model"
```

### Poetry Environment
Ensure you're in a poetry environment:
```bash
poetry install
poetry shell
```

### Import Errors
Make sure the source code is importable:
```bash
# Run from project root
cd /path/to/chives-backend
python -c "import chives_backend; import ai_core"
```

## ğŸ§¹ Code Quality Integration

The test runner can also run code quality checks:

```bash
# Check code formatting
poetry run black --check src/ tests/

# Check import sorting  
poetry run isort --check-only src/ tests/

# Run linting
poetry run flake8 src/ tests/

# Type checking
poetry run mypy src/
```

## ğŸ¯ Best Practices

1. **Isolation**: Each test is independent and doesn't rely on others
2. **Mocking**: External dependencies are mocked to prevent side effects
3. **Clear Names**: Test names describe what functionality is being tested
4. **Arrange-Act-Assert**: Tests follow a clear structure
5. **Edge Cases**: Tests cover both happy path and error scenarios
6. **Async Support**: Full support for testing async functionality

## ğŸ“ˆ Extending the Test Suite

When adding new features to chives-backend:

1. **Add Tests**: Create tests in the appropriate `test_*.py` file
2. **Update Fixtures**: Add new fixtures to `conftest.py` if needed
3. **Add Markers**: Use appropriate markers for categorization
4. **Document**: Update this README with new test coverage

## ğŸ”„ Continuous Integration

These tests are designed to be CI-friendly:

- No external dependencies (everything is mocked)
- Fast execution (most tests run in milliseconds)
- Comprehensive coverage of core functionality
- Clear pass/fail indicators

For CI systems, run:
```bash
poetry install
poetry run pytest tests/ --cov=chives_backend --cov=ai_core --cov-report=xml
```
